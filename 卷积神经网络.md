# 卷积神经网络

### **一.神经网络**

​	人的大脑在获取信息时，会将信息传递到大脑中的神经元中，信息经过许多神经元的计算和分析之后，我们就可以得到结果。

​	神经网络就是一种模仿人脑的计算模型，由很多代码版的“神经元”组成，分层连接、共同工作来处理数据和学习复杂的模式。它的核心目标就是从数据中“学习”并做出预测或分类。



![image](https://github.com/dusty033/cnn/raw/main/1.png)



神经网络就是仿造人脑中的神经元对于信息的处理模式而发明的，如下图所示：下图中的神经网络分为三个部分：分别为**输入层、隐藏层以及输出层。**

（1）绿色为输入层，图中输入了一个猫的图片。

（2）蓝色为隐藏层，可以将蓝色的小圆圈理解为人脑的神经元，将虚线理解为神经元之间的连接，神经网络在隐藏层中分析刚刚输入进来的图片是什么。

（3）黄色为输出层，通过隐藏层的分析最终在输出层输出了“这是一只猫的图片”。

​		

![image](https://github.com/dusty033/cnn/raw/main/2.png)

### 二、卷积神经网络

​	卷积神经网络（CNN)是神经网络中专门为图像处理和分析设计的一种神经网络，特别擅长识别图像中的模式，比如边缘、颜色、形状、行为等。

​	卷积神经网络由多个层次结构构成，每一层都有特定功能，主要包括卷积层（Convolutional Layer）、池化层（Pooling Layer）、激活层（Activation Layer）、全连接层（Fully Connected Layer）和归一化层（Normalization Layer）。这些层共同工作，提取和分析输入数据的特征。

​	卷积神经网络相比神经网络多了**“卷积**”二字，也代表着它使用**卷积操作**来处理图像以获得图像中的信息。那么首先，什么是卷积呢？



### 1.卷积

​	**卷积**（Conv）是深度学习中卷积神经网络（CNN）的核心操作，用于提取输入数据，卷积帮助网络识别图像中的局部特征，例如边缘、纹理和形状。

​	（1）从人类的视角中，卷积可以理解为一个小窗口（通常叫做**卷积核**）在图像上滑动，每次覆盖图像的一小部分区域，并与这部分区域的像素值相乘、求和，得出一个新的特征图。下图展示了一个基本的卷积操作。

​	 在这张图中，下方的蓝色的图是输入到卷积神经网络中的图片，上方的绿色图像则是通过卷积操作后所得到的绿色**特征图**。

![image](https://github.com/dusty033/cnn/raw/main/3.gif)

​     （2）从机器的视角中，卷积可以理解为**权值矩阵的数值计算**，如下图所示：

​	   机器的视角中，我们输入的图像的每一个像素点都是由权重组成的，所以对于机器来说，卷积操作就是针对于各个像素点的权值矩阵计算，通过多次计算，将一张图浓缩再浓缩，以学习其中的特征。具体的计算可以不需要学习。

![image](https://github.com/dusty033/cnn/raw/main/4.gif)



#### 1.1 卷积核

卷积核是一个小矩阵，他在图像上滑动来提取图片特征。

**卷积核**在这张图中就是下方蓝色图片中**深色的3乘3小矩阵（卷积核大小代码中缩写为K)**，它依次在图片上滑动，每滑动一次则进行一次特征提取，每次特征提取都会将下方图像中卷积核大小的部分（这里是3乘3）的特征浓缩成上方绿色图中1乘1的部分。

![image](https://github.com/dusty033/cnn/raw/main/3.gif)

（1）**卷积核的大小是可以自定义**的，比如可以是1乘1的，3乘3的，5乘5的，不同大小的卷积核可以提取图像中出不同的特征。

（2）**卷积核每一次不一定只走一步，步长（stride，代码中缩写为s）是可以自定义的**。

​	常见的步长是 1，这意味着卷积核每次只移动一个像素。如果步长是 2，那么卷积核每次会跳过一个像素，这样特征图的尺寸会比步长为 1 的特征图更小。

​	**步长越大**，特征图就越小，因为卷积核覆盖整个图像的速度更快，计算的像素点更少。



####  1.2 填充

 我们来思考一个问题，在下方的卷积中，仔细观察会发现，随着卷积核一次次移动，**蓝色图像中间的2乘2的正方体，每次卷积核移动的时候，它的特征都被记录了。而蓝色图像中的边缘被记录的次数显著少于中心部分**。这样就会造成一个问题：如果我们不做点什么，那么图像的边缘信息往往会被遗漏。

​	



![image](https://github.com/dusty033/cnn/raw/main/3.gif)

所以我们这里引入**填充（padding，代码中缩写为p）**，他的实现如下图所示：

可以看出，**通过将原始图片的特征矩阵外围补0的操作，将原始图像用0矩阵包裹起来，这样原始图像的特征矩阵就都变成了内部信息，可以被卷积核多次捕捉。**

![image](https://github.com/dusty033/cnn/raw/main/3.png)

（1）**填充不一定只采用0矩阵填充**，还有一些常见的填充方式，比如边缘填充（采用图像边缘的像素值进行填充）、复制填充（将图像的边缘像素进行复制填充，这种填充方式会产生扩展效果）

（2）**填充可以控制卷积得到的特征图的尺寸**，比如上边卷积操作中的3乘3卷积核每次卷积都会减少图像的宽和高，如果不采用填充，经过多次卷积，图像会迅速缩小，变成极小的特征图，从而丢失过多的信息。

​	采用填充操作就可以控制特征图的尺寸，甚至可以保持输入和输出的尺寸相同。**这样不仅可以让网络更深，还可以在不减小尺寸的前提下提取更多特征。**



#### 1.3 特征图大小的计算公式

​	目前已经学习了**卷积核（大小为K），步长（S），填充（P）**

​	假设输入图像大小为H乘W，则特征图尺寸可以计算为：

![image](https://github.com/dusty033/cnn/raw/main/4.png)

####  总而言之，卷积层提取图像中的局部特征，如边缘和形状，但随着卷积操作的进行，特征图的数量越来越多，参数也会越来越多，因此需要对特征图进行降采样操作，保留图像的主要特征的同时减少数据量。所以这里我们引入池化层





### 2.池化层

​	池化层（Pooling Layer）的作用是为了压缩卷积操作得到的特征图的尺寸，降低计算的复杂度，同时也能保留图像中关键的特征，因此可以将池化层理解为更高效提取和精简信息的作用。

#### 	 2.1 池化层的实现方法：

​		（1）**最大池化**（Max Pooling）：最大池化在一个小区域（如 2x2 或 3x3）内选取最大的数值作为该区域的代表。

​			首先不要忘了机器对于特征图采取的都是**矩阵操作**，假设我们有一个4乘4的特征图，应用2乘2的最大池化，步长为2，则最大池化的过程如图所示：

​			![image](https://github.com/dusty033/cnn/raw/main/5.png)





​			

​		























#### 	

