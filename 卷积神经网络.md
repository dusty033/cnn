

# 卷积神经网络

### **一.神经网络**

​	人的大脑在获取信息时，会将信息传递到大脑中的神经元中，信息经过许多神经元的计算和分析之后，我们就可以得到结果。

​	神经网络就是一种模仿人脑的计算模型，由很多代码版的“神经元”组成，分层连接、共同工作来处理数据和学习复杂的模式。它的核心目标就是从数据中“学习”并做出预测或分类。



![image](https://github.com/dusty033/cnn/raw/main/1.png)



神经网络就是仿造人脑中的神经元对于信息的处理模式而发明的，如下图所示：下图中的神经网络分为三个部分：分别为**输入层、隐藏层以及输出层。**

（1）绿色为输入层，图中输入了一个猫的图片。

（2）蓝色为隐藏层，可以将蓝色的小圆圈理解为人脑的神经元，将虚线理解为神经元之间的连接，神经网络在隐藏层中分析刚刚输入进来的图片是什么。

（3）黄色为输出层，通过隐藏层的分析最终在输出层输出了“这是一只猫的图片”。

​		

![image](https://github.com/dusty033/cnn/raw/main/2.png)

------

------



### 二、卷积神经网络

​	卷积神经网络（CNN)是神经网络中专门为图像处理和分析设计的一种神经网络，特别擅长识别图像中的模式，比如边缘、颜色、形状、行为等。

​	卷积神经网络由多个层次结构构成，每一层都有特定功能，主要包括**卷积层（Convolutional Layer）、池化层（Pooling Layer）、激活层（Activation Layer）、全连接层（Fully Connected Layer）和归一化层（Normalization Layer）。这些层共同工作，提取和分析输入数据的特征。**
如下图所示：![image](https://github.com/dusty033/cnn/raw/main/7.png)

​	卷积神经网络相比神经网络多了**“卷积**”二字，也代表着它使用**卷积操作**来处理图像以获得图像中的信息。那么首先，什么是卷积呢？



### 1.卷积层

​	**卷积**（Conv）是深度学习中卷积神经网络（CNN）的核心操作，用于提取输入数据，卷积帮助网络识别图像中的局部特征，例如边缘、纹理和形状。

​	（1）从人类的视角中，卷积可以理解为一个小窗口（通常叫做**卷积核**）在图像上滑动，每次覆盖图像的一小部分区域，并与这部分区域的像素值相乘、求和，得出一个新的特征图。下图展示了一个基本的卷积操作。

​	 在这张图中，下方的蓝色的图是输入到卷积神经网络中的图片，上方的绿色图像则是通过卷积操作后所得到的绿色**特征图**。

![image](https://github.com/dusty033/cnn/raw/main/3.gif)

​     （2）从机器的视角中，卷积可以理解为**权值矩阵的数值计算**，如下图所示：

​	   机器的视角中，我们输入的图像的每一个像素点都是由权重组成的，所以对于机器来说，卷积操作就是针对于各个像素点的权值矩阵计算，通过多次计算，将一张图浓缩再浓缩，以学习其中的特征。具体的计算可以不需要学习。

![image](https://github.com/dusty033/cnn/raw/main/4.gif)



#### 1.1 卷积核

卷积核是一个小矩阵，他在图像上滑动来提取图片特征。

**卷积核**在这张图中就是下方蓝色图片中**深色的3乘3小矩阵（卷积核大小代码中缩写为K)**，它依次在图片上滑动，每滑动一次则进行一次特征提取，每次特征提取都会将下方图像中卷积核大小的部分（这里是3乘3）的特征浓缩成上方绿色图中1乘1的部分。

![image](https://github.com/dusty033/cnn/raw/main/3.gif)

（1）**卷积核的大小是可以自定义**的，比如可以是1乘1的，3乘3的，5乘5的，不同大小的卷积核可以提取图像中出不同的特征。

（2）**卷积核每一次不一定只走一步，步长（stride，代码中缩写为s）是可以自定义的**。

​	常见的步长是 1，这意味着卷积核每次只移动一个像素。如果步长是 2，那么卷积核每次会跳过一个像素，这样特征图的尺寸会比步长为 1 的特征图更小。

​	**步长越大**，特征图就越小，因为卷积核覆盖整个图像的速度更快，计算的像素点更少。



####  1.2 填充

 我们来思考一个问题，在下方的卷积中，仔细观察会发现，随着卷积核一次次移动，**蓝色图像中间的2乘2的正方体，每次卷积核移动的时候，它的特征都被记录了。而蓝色图像中的边缘被记录的次数显著少于中心部分**。这样就会造成一个问题：如果我们不做点什么，那么图像的边缘信息往往会被遗漏。

​	



![image](https://github.com/dusty033/cnn/raw/main/3.gif)

所以我们这里引入**填充（padding，代码中缩写为p）**，他的实现如下图所示：

可以看出，**通过将原始图片的特征矩阵外围补0的操作，将原始图像用0矩阵包裹起来，这样原始图像的特征矩阵就都变成了内部信息，可以被卷积核多次捕捉。**

![image](https://github.com/dusty033/cnn/raw/main/3.png)

（1）**填充不一定只采用0矩阵填充**，还有一些常见的填充方式，比如边缘填充（采用图像边缘的像素值进行填充）、复制填充（将图像的边缘像素进行复制填充，这种填充方式会产生扩展效果）

（2）**填充可以控制卷积得到的特征图的尺寸**，比如上边卷积操作中的3乘3卷积核每次卷积都会减少图像的宽和高，如果不采用填充，经过多次卷积，图像会迅速缩小，变成极小的特征图，从而丢失过多的信息。

​	采用填充操作就可以控制特征图的尺寸，甚至可以保持输入和输出的尺寸相同。**这样不仅可以让网络更深，还可以在不减小尺寸的前提下提取更多特征。**



#### 1.3 特征图大小的计算公式

​	目前已经学习了**卷积核（大小为K），步长（S），填充（P）**

​	假设输入图像大小为H乘W，则特征图尺寸可以计算为：

![image](https://github.com/dusty033/cnn/raw/main/4.png)

#### 总而言之，卷积层提取图像中的局部特征，如边缘和形状，但都是简单的线性计算，无法处理复杂的关系，所以我们这里需要引入激活层，帮助模型学习多样化的特征。



----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

------



### 2.激活层

​	激活层概念相对来讲有点抽象，它的主要任务是用来**引入非线性，是模型能够学会复杂的模式和非线性关系**。举个通俗易懂的例子，假设你想要画一只猫的轮廓。如果不用激活层，网络就只能画出“直线”或“简单图形”，所以结果可能像一个正方形。但是激活层的存在，就相当于给网络“上色”和“弯曲”的能力，让它可以画出耳朵、胡须，甚至弯曲的尾巴。这就是激活层引入的非线性所起的作用。



#### 2.1 常见的激活函数

​	常见的激活函数包括 **ReLU**、**Sigmoid**、**Tanh** 和 **Leaky ReLU**。ReLU 是最常用的，将负数变成 0，计算快，适合深层网络，但可能导致一些神经元“死亡”；Sigmoid 则将值压缩在 0 到 1 之间，适合二分类问题，但容易导致训练变慢；Tanh 把值压缩在 -1 到 1 之间，比 Sigmoid 更适合平衡数据，但也有梯度消失的问题。**（了解即可）**



#### 总而言之，卷积层提取图像中的局部特征，如边缘和形状，激活层引入非线性，让模型能学会复杂的模式，但随着激活函数的进行，由于函数的特性，部分数据会产生极值，这会导致数据范围变得不平衡，这时候就需要一个网络结构来调整数据范围，使模型变得更稳定，这就需要归一化层。



------

------



### 3.归一化层

​	**归一化层**的作用是让每一层的输出数据更“均匀”，避免因激活函数导致的极端值，使模型更稳定、训练更快。它就像在每一层做“校准”，把数据调整到合理的范围，以便下一层更好地学习。



#### 3.1 归一化的工作原理（了解即可）

​	在卷积神经网络中，归一化层的工作原理是先对一个批次的特征图计算每个通道的平均值和波动情况（即方差），然后将特征图中的数值“标准化”——让它们围绕0上下波动，保持在合理的范围内。



#### 总而言之，卷积层提取图像中的局部特征，如边缘和形状，激活层引入非线性，让模型能学会复杂的模式，归一化层将数据校准。但随着卷积操作和激活函数的进行，特征图的数量越来越多，参数也会越来越多，因此需要对特征图进行降采样操作，保留图像的主要特征的同时减少数据量。所以这里我们引入池化层。



------

------



### 4.池化层

​	池化层（Pooling Layer）的作用是为了压缩卷积操作得到的特征图的尺寸，降低计算的复杂度，同时也能保留图像中关键的特征，因此可以将池化层理解为更高效提取和精简信息的作用。



#### 	 4.1 池化层的实现方法：

​	（1）**最大池化**（Max Pooling）：最大池化在一个小区域（如 2x2 或 3x3）内选取最大的数值作为该区域的代表。

​			这种方法有助于提取图像中的显著特征，尤其是边缘、角等重要特征。保留区域内的最大值可以帮助模型更好地捕捉到强烈的特征。在大多数图像任			务中，**最大池化是最常见的池化方式**，因为它能有效地保留特征图中的重要信息。**最大池化具体操作如下图所示：**

​			首先不要忘了机器对于特征图采取的都是**矩阵操作**，假设我们有一个4乘4的特征图，应用2乘2的最大池化，步长为2，则最大池化的过程如图所示：

​			![image](https://github.com/dusty033/cnn/raw/main/5.png)

​		（2）**平均池化**（Average Pooling）：平均池化会**计算池化区域内所有数值的平均值**，并将其作为该区域的代表。

​		          平均池化可以保留区域的整体信息，使模型对细节更加平滑和平均。相比最大池化，它在某种程度上会平滑图像特征，但会牺牲一些局部的显著特征。

​	       （3）**全局池化**（Global Pooling）：全局池化会将**整个特征图中的数值通过最大值或平均值合并成一个数**。

​			  全局池化将整个特征图缩小为一个单一的数值，从而减少数据维度，常用于分类任务的最后一个卷积层之后。

####  

#### 总而言之，经过了池化层的降采样后，关键特征被保留，数据量也得以减少，这个时候就需要把网络学习到的特征整合起来，合并成最后的输出了！这个时候我们就需要采用全连接层进行操作。





------

------



### 5.全连接层

​	**全连接层**（Fully Connected Layer, FC）是神经网络的最后几层，用于综合前面各层提取到的特征并生成最终的输出。在全连接层中，每个神经元都与上一层的所有神经元相连，这种完全连接的结构能将所有特征整合在一起，用于完成分类、回归等任务。



#### 	5.1 全连接层的工作原理

​		首先，如4.1中举得全局池化的例子所示，池化层的输出结果往往是一个二维或者三维的矩阵向量，而全连接层会将这个矩阵向量特征摊平乘一个一维向量，是数据结构更适合分类或预测任务。具体例子如下图所示：

​		将二维特征图摊平成一维向量后，特征就可以输入到全连接层。全连接层将使用这个一维向量中的所有值来进行加权求和，最终输出一个结果，用于分类或预测等任务。

![image](https://github.com/dusty033/cnn/raw/main/6.png)

​		**全连接层通常采用使用 Softmax（分类任务）或线性函数（回归任务）生成最终的结果**。Softmax 会将输出转换为概率分布，帮助模型做出分类决策。**动作识别就是分类任务，所以采用softmax******（了解即可）****

#### 	

**总而言之，全连接层后，就可以输出类别啦！**

------

------











#### 	

